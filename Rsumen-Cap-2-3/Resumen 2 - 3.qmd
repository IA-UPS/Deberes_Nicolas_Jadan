---
title: "Resumen Cap 2/3"
format: pdf
editor: visual
---

**Integrantes:** María José Bustamante - Nicolás Jadán

**Carrera:** Biomedicina

# CAPÍTULO 2

# Aprendizaje estadístico

## ¿Qué es el aprendizaje estadístico?

Ayuda a determinar si existe una relación entre las variables de interés para su futura aplicación.

Variable de entrada (input): Suelen denotarse como "X" y con un subídice para diferenciarlas. Se conocen como; predictores, características y variables independientes.

Variable de salida (output): Se denota como "Y". Conocida generalmente como variable dependiente o de respuesta.

El aprendizaje estadístico se refiere a un conjunto de enfoques para estimar *f* que es una función fija pero desconocida que representa información sistemática que brinda X acerca de Y.

### ¿Cómo estimar *f*?

**Predicción**

Generalmente se suele tener los datos de entrada pero la parte difícil es obtener la salida. Esto se puede solucionar usando:

![](images/predicci%C3%B3n.png){fig-align="center" width="82" height="34"}

f: estimación para f

y: predicción resultante

Un ejemplo se muestra en la siguiente imagen:

![](images/imagen%20prediccion.png){fig-align="center" width="283"}

El gráfico muestra los ingresos en función de los años de educación y antigüedad en el conjunto de datos Renta. La superficie azul representa la verdadera relación subyacente entre los ingresos y los años de educación y antigüedad, que se conoce porque los datos son simulados. Los puntos rojos indican los valores de estas cantidades para 30 individuos.

La exactitud de y depende de dos tipos de error:

-   Error reducible: Se refiere a la precisión de las predicciones que puede mejorar implementando mejores algoritmos para estimar f(X).

-   Error irreducible: Aunque fuera posible obtener la mejor estimación de f(X), seguirá existiendo un cierto nivel de incertidumbre ya que siempre existen dependencias de la variable objetivo con otras variables que no se están considerando o simplemente por procesos debidos al azar. Esto es lo que se conoce como error irreducible. Siempre proporcionará un límite superior en la precisión de la predicción para Y.

**Inferencia**

Para comprender la asociación entre Y y Xp se pueden plantear las siguientes preguntas:

1.  *¿Qué predictores se asocian a la respuesta?*

2.  *¿Cuál es la relación entre la respuesta y cada predictor?*

3.  *¿Puede resumirse adecuadamente la relación entre Y y cada predictor mediante una ecuación lineal, o la relación es más complicada?*

Algunos modelos podrían utilizarse tanto para la predicción como para la inferencia, dependiendo delobjetivo final. Por ejemplo, los modelos lineales permiten una inferencia relativamente sencilla y predecible entre modelos lineales, pero puede que no produzcan predicciones tan precisas como otros enfoques. Por el contrario, algunos de los enfoques no lineales pueden brindar información bastante precisas para Y.

## ¿Cómo estimamos f?

**Métodos paramétricos**: Abarcan un enfoque basado en modelos de dos pasos:

1.  Suposición sobre la forma funcional para el modelo. Ej: f es lineal en X.

2.  Después de seleccionar el modelo se requiere de un procedimiento que utilice los datos de entrenamiento para ajustar o entrenar el modelo.

El método más común para ajustar un modelo es el de mínimos cuadrados.

Ejemplo:

![*Modelo lineal ajustado por mínimos cuadrados a datos de ingresos. Las observaciones se muestran en rojo, y el plano amarillo indica el ajuste por mínimos cuadrados a los datos.*](images/lineal%20mod.png){fig-align="center" width="377"}

El modelo paramétrico reduce el problema para estimar f ya que se presenta como un conjunto de parámetros en el modelo lineal caso contario se debería ajustar f a una función arbitraria.

La desventaja de este modelo es que cuando el modelo elegido se aleje demasiado la estimación será deficiente y para resolver esto se debería estimar una mayor cantidad de parámetros lo que a su vez podría provocar un sobreaajuste de datos.

**Métodos no paramétricos:** Buscan una estimación de f que se acerque lo más posible a los puntos de datos sin que sea demasiado aproximada. Se ajustan fácilmente a una gama más amplia de formas posibles de f.

Una desventaja que presentan es que se necesita un gran número de observaciones para obtener una estimación precisa de f.

## El equilibrio entre la precisión de la predicción y la interpretabilidad del modelo

Existen métodos menos flexible o menos restrictivos en el sentido de que sólo pueden producir una gama relativamente pequeña de formas para estimar f.

![*Representación de la relación entre flexibilidad e interpretabilidad, utilizando diferentes métodos de aprendizaje estadístico. En general, a medida que aumenta la flexibilidad de un método, disminuye su interpretabilidad.*](images/flexibilidad.png){fig-align="center" width="467"}

*Modelo restrictivo:* Es útil cuando el principal interés es la inferencia, debido a que es más interpretable.

*Modelos flexibles:* Pueden guiar a estimaciones muy complicadas de f en las que es difícil comprender cómo se asocia cualquier predictor con la respuesta.

-   lasso: es un enfoque menos flexible y más interpretable que la regresión lineal proque en el modelo final la respuesta sólo estará relacionada con el modelo final.

-   Modelos aditivos generalizados (GAM): Más felxibles que la regresión lineal, pero menos interpretables ya que la relación entre cada predictor y respuesta se representa mediante una curva.

-   Modelos no lineales: bagging, boosting, máquinas de soporte de vectores y redes neuronales.

## Aprendizaje supervisado frente a aprendizaje no supervisado

Supervisado: Para cada observación de los predictores hay una respesta asociada. Permite predecir con exactitud la prespuesta para futuras predicciones o comprender mejor la relaxción entre predictores y respuesta.

No supervisado: Se carece de una variable de respuesta que pueda supervisar el análisis. Es decir no hay una respuesta asociada al predictor, por lo que no e sposible ajustar a un modelo de regresión lineal.

## Problemas de regresión frente a problemas de clasificación

Variables cuantitativas: Toman valores numéricos. Ej: Estatura, edad o ingresos.

Variables cualitativas: Toman valores en clases o categorías. Ej: Estado cívil, marcas de productos o diagnósticos.

La regresión logística es un método de clasificación binaria. Es bastante común seleccionar los métodos de aprendizaje estadístico en función de si la respuesta es cuantitativa o cualitativa, es decir, se puede usar la regresión lineal cuando es cuantitativa y la regresión logística cuando es cualitativa.

# Evaluación de la precisión de los modelos

En en estadística: ningún método domina a todos los demás en todos los conjuntos de datos posibles.

## Medir la calidad del ajuste

Para evaluar el rendimiento de un método de aprendizaje estadístico se requiere cuantificar hasta qué punto el valor de respuesta predicho para una observación dada se aproxima al valor de respuesta verdadero para esa observación.

*Error cuadrático medio (MSE):* será pequeño si las respuestas predichas están muy cerca de las respuestas verdaderas, y será grande si para algunas de las observaciones, las respuestas predichas y verdaderas difieren sustancialmente.

Se calcula utilizando los datos de entrenamiento que se usaron para jaustar el modelo

![***Izquierda:** Datos simulados a partir de f, mostrados en negro. Se muestran tres estimaciones de f: la línea de regresión lineal (curva naranja) y dos splines de suavizado (curvas azul y verde). (curvas azul y verde). **Derecha:** MSE de entrenamiento (curva gris), MSE de prueba (curva roja roja) y el MSE de prueba mínimo posible de todos los métodos (línea discontinua). Los cuadrados MSE de entrenamiento y prueba de los tres ajustes mostrados en el panel izquierdo.*](images/MSE.png){fig-align="center" width="481"}

En la imagen en la parte izquierda se puede ver que a medida que aumenta el nivel de flexibilidad, las curvas se ajustan mejor a los datos observados. La curva verde es la más flexible y se ajusta muy bien a los datos; sin embargo, observamos que se ajusta mal a la f verdadera (mostrada en negro) porque es demasiado ondulada.

En la parte derecha La curva gris muestra el MSE medio de entrenamiento en función de la flexibilidad, o más formalmente de los grados de libertad (flexibilidad de la curva). Los cuadrados naranja, azul y verde indican los MSE asociados a las curvas.

En este ejemplo, la verdadera f no es lineal, por lo que el ajuste lineal naranja no es lo suficientemente flexible para estimar bien f. La curva verde tiene el MSE de entrenamiento más bajo de los tres métodos, ya que corresponde al más flexible de ellos. El spline de suavizado representado por la curva azul se aproxima al óptimo.

En la parte derecha de la figura, a medida que aumenta la flexibilidad del método de aprendizaje estadístico, observamos un descenso monótono en el tiempo de entrenamiento. Es decir, a medida que aumenta la flexibilidad del modelo aumenta, el MSE de entrenamiento disminuirá.

Cuando un método determinado produce un MSE de entrenamiento pequeño pero un MSE de prueba grande, se dice que se están sobreajustando los datos.

## La relación entre sesgo y varianza

Para minimizar el error de prueba esperado se requiere seleccionar un método de aprendizaje estadístico que consiga simultáneamente baja varianza y bajo sesgo.

Varianza: Se refire a la cantidad en la que *f* cambiaría si la estimación se realizara usando un conjunto de datos de enttrenamiento diferente. Lo ideal es que la estimación de f no varíe demasiado. En general los métodos estadísticos más flexibles tienen mayor varianza.

Sesgo: Es el error que se introduce al aproximar un problema de la vida real a un modelo muy simple o sencillo. Es poco probable que un problema de la vida real tenga una relación lineal tan sencilla, por lo que realizar una regresión lineal dará lugar a cierto sesgo en la estimación de f.

La varianza es intrínsecamente una cantidad no negativa, y el sesgo al cuadrado también es no negativo.

A medida que utilicemos métodos más flexibles, la varianza aumentará y el sesgo disminuirá.

Un buen rendimiento del conjunto de prueba de un método de aprendizaje estadístico requiere una varianza baja, así como un equilibrio entre sesgo y varianza.

## El entorno de clasificación

Las tasas de error resultantes son de especial interés para la aplicación del clasificador a observaciones de prueba que no fueron utilizadas en el entrenamiento. Un buen clasificador es aquel en el que el error de prueba es mínimo.

### El clasificador de Bayes

Asigna cada observación a la clase más probable dados sus valores predictores. En un problema de dos clases en el que sólo hay dos posibles valores de respuesta, el clasificador de Bayes corresponde a la predicción de la clase uno si Pr(Y = 1\|X = x0) \> 0,5, y la clase dos en caso contrario.

![*Un conjunto de datos simulados compuesto por 100 observaciones en cada uno de dos grupos, indicados en azul y en naranja. La línea discontinua morada representa el límite de decisión de Bayes. La cuadrícula de fondo naranja indica la región en la que una observación de prueba se asignará a la clase naranja, y la cuadrícula de fondo azul azul indica la región en la que una observación de prueba se asignará a la clase azul. a la clase azul.*](images/bayes.png){fig-align="center" width="411"}

En la imagen los círculos naranja y azules corresponden a observaciones de entrenamiento que pertenecen a dos clases diferentes.

Para cada valor de X1 y X2, existe una probabilidad diferente de que la respuesta sea naranja o azul.

La región sombreada en naranja refleja el conjunto de puntos para los que Pr(Y = naranja\|X) es superior al 50 %, mientras que la región sombreada en azul indica el conjunto de puntos cuya probabilidad es inferior al 50 %.

La línea discontinua morada representa los puntos en los que la probabilidad es exactamente del 50 %. Esto se denomina el **límite de decisión de Bayes.**

### K-Nearest Neighbors

Es utilizado para trabajar con datos reales. Es un algoritmo no supervisado donde "K" representa el número de "grupos" (clusters) a clasificar y el K-neighbor más cercano "K" representa el número de "vecinos" considerados en los "n" grupos del clasificador. En otras palabras busca en las observaciones más cercanas a la que se está tratando de predecir y clasifica el punto de interés basado en la mayoría de datos que le rodean.

![*Comparación de los límites de decisión KNN (curvas negras obtenidas utilizando K = 1 y K = 100 en los datos de la Figura 2.13. Con K = 1, el límite de decisión es demasiado flexible, mientras que con K = 100 no es suficientemente flexible. El límite de decisión de Bayes se muestra como una línea discontinua morada.*](images/k-neig.png){fig-align="center" width="446"}

La imagen muestra dos ajustes KNN a los datos simulados, utilizando K = 1 y K = 100. Cuando K = 1, el límite de decisión es excesivamente flexible y encuentra patrones en los datos que no se corresponden con el límite de decisión de Bayes. Esto corresponde a un clasificador que tiene un sesgo bajo pero una varianza muy alta. A medida que K aumenta, el método se vuelve menos flexible y produce una frontera de decisión cercana a la lineal. Esto corresponde a un clasificador de baja varianza pero alto sesgo. En este conjunto de datos simulados, ni K = 1 ni K = 100 dan buenas predicciones: tienen tasas de error de prueba de 0,1695 y 0,1925, respectivamente.

# LAB: CAP 3 - Regresión linear

Librerías a usar:

MASS: colección de conjunto de datos y funciones.

ISLR2: Conjunto de datos asociados al libro de estudio.

```{r}
library (MASS)
library (ISLR2)
```

## Regresión linear simple

La biblioteca ISLR2 contiene el conjunto de datos de Boston, que registra el medv (valor medio de la vivienda) de 506 secciones censales de Boston. Intentaremos predecir medv utilizando 12 predictores como rm (número medio de habitaciones por casa) edad (edad media de las casas) y lstat (porcentaje de hogares con un estatus socioeconómico bajo).

```{r}
head(Boston)
```

-   **?Boston:** Para obtener información sobre el conjunto de datos.

-   **lm():** Para ajustar un modelo de regresión lineal simple.

-   **Predictor:** Istat

-   **Respuesta:** medv

La sintaxis básica es lm(y ∼ x, datos), donde y es la respuesta, x es el predictor y datos es el conjunto de datos en el que se mantienen estas dos variables.

```{r}
attach(Boston)
lm.fit<-lm(medv~lstat,data = Boston)
lm.fit<-lm(medv~lstat)
```

Si escribimos lm.fit, aparecerá información básica sobre el modelo.

Para obtener información más detallada, utilice summary(lm.fit). Se obtienen los valores p y los errores estándar de los coeficientes, así como el estadístico R2 y el estadístico F del modelo.

```{r}
lm.fit
summary(lm.fit)
```

Podemos utilizar la función names() para averiguar qué otras piezas nombres de información se almacenan en lm.fit.

Es más seguro utilizar las funciones extractoras como coef() para acceder a ellas.

```{r}
names(lm.fit)
coef(lm.fit)
```

confint(): Para obtener un intervalo de confianza para las estimaciones de los coeficientes.

```{r}
confint(lm.fit)
```

predict(): Usada para producir intervalos de confianza para la predicción de medv para un valor dado de lstat.

```{r}
predict(lm.fit,data.frame(lstat=(c(5,10,15))),interval = "confidence")
predict(lm.fit,data.frame(lstat=(c(5,10,15))),interval = "prediction")
```

Por ejemplo, el intervalo de confianza del 95 % asociado a un valor lstat de 10 es (24.47, 25.63), y el intervalo de predicción del 95 % es (12.828, 37.28). Como era de esperar, los intervalos de confianza y predicción se centran en el mismo punto (un valor predicho de 10). En el mismo punto (un valor predicho de 25,05 para medv cuando lstat es igual a 10), pero estos últimos son mucho más amplios.

Ahora trazaremos medv y lstat junto con la línea de regresión por mínimos cuadrados utilizando las funciones plot() y abline(). **abline():** se puede usar para agregar líneas verticales, horizontales o de regresión a un gráfico.

```{r}
plot(lstat,medv)
abline(lm.fit)
```

Se puede observar indicios de no linealidad en la relación entre lstat y medv.

En la función abline() para dibujar una recta con intersección en a y pendiente b, se escribe **abline(a,b)**. Unos ajustes adicionales para trazar líneas y puntos son:

-   lwd: Para la anchura de la línea de regresión.

-   pch: Para crear diferentes símbolos de trazado.

```{r}
plot(lstat,medv)
abline (lm.fit , lwd = 3)
abline (lm.fit , lwd = 3, col = " red ")
plot (lstat , medv , col = " red ")
plot (lstat , medv , pch = 20)
plot (lstat , medv , pch = "+")
plot (1:20, 1:20, pch = 1:20)
```

par() y mfrow(): Indicaan a R que divida la pantalla de vizualización en paneles separados para poder visualizar gráficos simultaneamente.

```{r}
plot(lm.fit)
par(mfrow=c(2,2))
```

residuals (): Para calcular los residuos de un ajuste de regresión lineal.

rstudent(): Devuelve los residuales estudiados y se puede usar para representar gráficamente los residuos frente a los valores ajustados

```{r}
plot ( predict (lm.fit), residuals (lm.fit))
plot ( predict (lm.fit), rstudent (lm.fit))
```

hatvalues(): Sirve para calcular los estadísticos utilizando cualquier número de predictores.

which max: identifica el índice del elemento mayor de un vector. En este caso dice que la observación tiene el mayor estadístico de apalancamiento (375).

```{r}
plot ( hatvalues (lm.fit))
which.max( hatvalues (lm.fit))
```

## Regresión lineal múltiple

Para ajustar un modelo de regresión lineal múltiple por mínimos cuadrados, utilizamos de nuevo la función lm(). La sintaxis lm(y ∼ x1 + x2 + x3) se utiliza para ajustar un modelo con tres predictores, x1, x2 y x3.

```{r}
lm.fit<-lm(medv~lstat+age,data=Boston)
summary(lm.fit)
```

El conjunto Boston contiene 12 variables por lo que sería más tardado escribir todas las variables, en su lugar se utiliza:

```{r}
lm.fit<-lm(medv~.,data = Boston)
summary(lm.fit)
```

Para acceder a los componentes específicos de un elemento se escribe: `summary(lm.fit)$sigma`

vif(): utilizada para calcular los factores de inflación de la varianza. Para esto es necesario usar la siguiente librería:

```{r}
library(car)
vif(lm.fit)
```

Para utilizar todas la variables menos una:

```{r}
lm.fit1<-lm(medv~.-age,data = Boston)
summary(lm.fit1)
```

Como alternativa, puede utilizarse la función update()

```{r}
lm.fit1<-update(lm.fit,~.-age)
```

## Términos de interacción

lstat:black: Indica a R que debe incluir un término de interacción entre lstat y black.

lstat\*age: incluye simultaneamente lstat, age, y el término de interacción lstat x age como predictores.

```{r}
summary(lm(medv~lstat*age,data = Boston))
```

## Transformaciones no lineales de los predictores

lm puede ser ajustada a transformaciones no lineales de los predictores. La función I() es necesaria ya que el \^ tiene un significado especial I() en un objeto de fórmula; permite el uso estándar en R, que es elevar X a la potencia 2. Ahora realizamos una regresión de medv sobre lstat y lstat2.

```{r}
lm.fit2<-lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
```

El valor p casi nulo asociado al término cuadrático sugiere modelo mejorado.

anova(): Sirve para cuantificar mejor hasta qué punto el ajuste cuadrático es superior al lineal.

```{r}
lm.fit<-lm(medv~lstat)
anova(lm.fit,lm.fit2)
```

Modelo 1: Representa el submodelo lineal que tiene un solo predictor (lstat).

Modelo 2: Modelo cuadrático más amplico con dos predictores (lstat y lstat\^2).

La función anova realiza una pruea de hipótesis que compara los dos modelos.

Una hipótesis nula quiere decir que los dos modelos se ajustan correctamente a los datos.

Una hipótesis alternativa significa que el modelo es superior.

En este caso, el estadístico F es 135 y el valor p asociado es prácticamente cero. Esto demuestra claramente que el modelo que contiene los predictores lstat y lstat2 es muy superior al modelo que sólo contiene el predictor lstat. Esto quiere decir que no existe linelidad.

```{r}
plot(lm.fit2)
par(mfrow=c(2,2))
```

Cuando se incluye lstat2 en el modelo se puede observar que hay pocos patrones dicernibles en los residuos.

Par crear un ajuste mayor se puede utilizar la función **poly()**.

```{r}
lm.fit5<-lm(medv~poly(lstat,5))
summary(lm.fit5)
```

Incluir términos de hasta quinto orden mejora el ajuste del modelo, pero si son mayores a este orden el modelo no tendrá valores p significativos en el ajuste de regresión.

Para obtener los polinomios brutos de la función poly(), debe utilizarse el argumento raw = TRUE.

**Tranformación logarítmica:**

```{r}
summary(lm(medv~log(rm),data = Boston))
```

## Predictores cualitativos

Para esta parte se usará: Carseats data para intentar predecir las ventas de asientos de autos en 4000 localidades basado en una serie de predictores.

```{r}
head(Carseats)
```

Predictores cualitativos incluidos: Shelveloc (calidad de la ubicación de la estantería). Tiene 3 valores posibles: Malo, medio y bueno. A continuación ajustamos un modelo de regresión múltiple que incluye algunos términos de interacción.

```{r}
lm.fit<-lm(Sales~.+Income:Advertising+Price:Age,data = Carseats)
summary(lm.fit)
```

contrasts() devuelve la codificación que R utiliza para las variables ficticias.

```{r}
attach(Carseats)
contrasts(ShelveLoc)
```

ShelveLocGood que toma un valor de 1 si la ubicación de la estantería es buena, y 0 en caso contrario.

El hecho de que el coeficiente de ShelveLocGood en el resultado de la regresión sea positivo indica que una buena estanterías se asocia a ventas elevadas (en comparación con las malas).

ShelveLocMedium tiene un coeficiente positivo menor, lo que indica tiene ubicación media de las estanterías se asocia a mayores ventas que a una mala ubicación, pero con menores ventas que una buena ubicación.

## Funciones de escritura

```{r}
LoadLibraries <- function () {
library (ISLR2)
library (MASS)
print ("The libraries have been loaded .")
}
```

Ahora si se escribe LoadLibraries, R dirá qué contiene la función.

```{r}
LoadLibraries
```

Si llamamos a la función, las librerías se cargan y la sentencia print se imprime.

```{r}
LoadLibraries()
```
